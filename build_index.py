# build_index.py  (CSV only, cosine, batch-encode)
import os, csv, re
from pathlib import Path
from typing import List, Dict, Tuple

import chromadb
from sentence_transformers import SentenceTransformer

# ---- paths / constants ----
DATA_DIR    = Path("data")
CSV_PATH    = DATA_DIR / "faq.csv"                 # à¸•à¹‰à¸­à¸‡à¸¡à¸µà¸„à¸­à¸¥à¸±à¸¡à¸™à¹Œ: Question, Answer
INDEX_PATH  = os.getenv("INDEX_PATH", "index")
COLL_NAME   = os.getenv("COLL_NAME", "fit_faq")

EMB_MODEL      = os.getenv("EMB_MODEL", "all-MiniLM-L6-v2")
EMBED_BATCH    = int(os.getenv("EMBED_BATCH", 64))         # à¸¥à¸”à¹€à¸›à¹‡à¸™ 32 à¸–à¹‰à¸² RAM à¸™à¹‰à¸­à¸¢
NORMALIZE_EMB  = os.getenv("NORMALIZE_EMB", "1") == "1"    # à¹ƒà¸Šà¹‰ normalization

# ---- init ----
client   = chromadb.PersistentClient(path=INDEX_PATH)
embedder = SentenceTransformer(EMB_MODEL)


def clean_text(s: str) -> str:
    s = s.replace("\u00ad", "")                 # soft hyphen
    s = re.sub(r"[ \t]+", " ", s)
    s = re.sub(r"\n{2,}", "\n\n", s)
    return s.strip()


def load_csv_faq(path: Path) -> List[Tuple[str, Dict]]:
    """
    à¸„à¸·à¸™à¸„à¹ˆà¸² list à¸‚à¸­à¸‡ (document_text, meta)
    document_text = Question + 2 newlines + Answer   <-- à¸Šà¹ˆà¸§à¸¢à¹ƒà¸«à¹‰ retrieval à¹à¸¡à¹ˆà¸™à¸‚à¸¶à¹‰à¸™
    meta['question'] à¹€à¸à¹‡à¸šà¹„à¸§à¹‰à¹ƒà¸Šà¹‰à¸­à¹‰à¸²à¸‡à¸­à¸´à¸‡ [Q#] à¸•à¸­à¸™à¸•à¸­à¸š
    """
    if not path.exists():
        raise FileNotFoundError(f"CSV not found: {path}")

    rows: List[Tuple[str, Dict]] = []

    # à¸­à¹ˆà¸²à¸™ header à¸”à¹‰à¸§à¸¢ csv.reader à¸à¹ˆà¸­à¸™ à¹€à¸žà¸·à¹ˆà¸­ control à¸Šà¸·à¹ˆà¸­à¸„à¸­à¸¥à¸±à¸¡à¸™à¹Œà¹€à¸­à¸‡
    with path.open("r", encoding="utf-8-sig", newline="") as f:
        raw_reader = csv.reader(f)
        try:
            header = next(raw_reader)
        except StopIteration:
            raise ValueError(f"CSV is empty: {path}")

        # à¸¥à¹‰à¸²à¸‡à¸Šà¸·à¹ˆà¸­à¸„à¸­à¸¥à¸±à¸¡à¸™à¹Œ: à¸•à¸±à¸” BOM + à¸•à¸±à¸” space
        clean_header = []
        for col in header:
            col = str(col).replace("\ufeff", "").strip()
            clean_header.append(col)

        required = {"Question", "Answer"}
        if not required.issubset(set(clean_header)):
            raise ValueError(
                f"CSV must contain 'Question' and 'Answer' columns. "
                f"Got: {clean_header}"
            )

        # à¹ƒà¸Šà¹‰ DictReader à¸•à¹ˆà¸­à¸ˆà¸²à¸à¸•à¸³à¹à¸«à¸™à¹ˆà¸‡à¹„à¸Ÿà¸¥à¹Œà¸›à¸±à¸ˆà¸ˆà¸¸à¸šà¸±à¸™ (à¸«à¸¥à¸±à¸‡ header à¹à¸¥à¹‰à¸§)
        rdr = csv.DictReader(f, fieldnames=clean_header)

        for r in rdr:
            # à¸•à¸­à¸™à¸™à¸µà¹‰ r["Question"], r["Answer"] à¸ˆà¸°à¹ƒà¸Šà¹‰ header à¸—à¸µà¹ˆà¸¥à¹‰à¸²à¸‡à¹à¸¥à¹‰à¸§
            q_raw = (r.get("Question") or "").strip()
            a_raw = (r.get("Answer") or "").strip()

            q = clean_text(q_raw)
            a = clean_text(a_raw)

            if not q or not a:
                # à¸‚à¹‰à¸²à¸¡à¹à¸–à¸§à¸§à¹ˆà¸²à¸‡à¸«à¸£à¸·à¸­à¹„à¸¡à¹ˆà¸„à¸£à¸š
                continue

            doc = f"{q}\n\n{a}"  # à¸£à¸§à¸¡ Q + A à¹€à¸›à¹‡à¸™ document à¹€à¸”à¸µà¸¢à¸§
            meta = {
                "source": path.name,
                "question": q,
                "type": "faq",
            }
            rows.append((doc, meta))

    return rows


def main():
    print(f"ðŸ§¹ Recreating collection at '{INDEX_PATH}' â€¦")
    try:
        client.delete_collection(name=COLL_NAME)
    except Exception:
        # à¸–à¹‰à¸²à¹„à¸¡à¹ˆà¸¡à¸µ collection à¹€à¸”à¸´à¸¡à¸­à¸¢à¸¹à¹ˆ à¸à¹‡à¸›à¸¥à¹ˆà¸­à¸¢à¸œà¹ˆà¸²à¸™
        pass

    # à¸šà¸±à¸‡à¸„à¸±à¸š cosine à¹€à¸ªà¸¡à¸­ (à¹ƒà¸«à¹‰à¹€à¸‚à¹‰à¸²à¸„à¸¹à¹ˆà¸à¸±à¸š retrieval.py)
    coll = client.get_or_create_collection(
        name=COLL_NAME,
        metadata={"hnsw:space": "cosine"},
    )

    items = load_csv_faq(CSV_PATH)
    if not items:
        print("âš ï¸ No rows found in CSV.")
        return

    docs  = [t for t, _ in items]
    metas = [m for _, m in items]
    ids   = [f"csv-{i+1}" for i in range(len(items))]

    print(f"ðŸ“„ CSV rows: {len(items)}")
    print(f"ðŸ§  Embedding with {EMB_MODEL} (batch={EMBED_BATCH}) â€¦")

    # ---- batch embeddings ----
    embs: List[List[float]] = []
    for i in range(0, len(docs), EMBED_BATCH):
        batch = docs[i:i+EMBED_BATCH]
        vecs = embedder.encode(
            batch,
            normalize_embeddings=NORMALIZE_EMB,
            show_progress_bar=False,
        )
        # SentenceTransformer à¸„à¸·à¸™à¹€à¸›à¹‡à¸™ np.array â†’ à¹à¸›à¸¥à¸‡à¹€à¸›à¹‡à¸™ list
        embs.extend(vecs.tolist())

    # ---- upsert ----
    coll.add(documents=docs, metadatas=metas, ids=ids, embeddings=embs)

    # à¸•à¸£à¸§à¸ˆà¸™à¸±à¸šà¸£à¸²à¸¢à¸à¸²à¸£à¸ˆà¸£à¸´à¸‡
    try:
        n = coll.count()
        print(f"âœ… Index built at ./{INDEX_PATH} with {n} items")
    except Exception:
        print(f"âœ… Index built at ./{INDEX_PATH} with {len(docs)} items")


if __name__ == "__main__":
    main()